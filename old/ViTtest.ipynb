{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688dcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train samples: 60000, Test samples: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MNIST Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x150528 and 784x28)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 203\u001b[39m\n\u001b[32m    200\u001b[39m data, target = data.to(device), target.to(device)\n\u001b[32m    202\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m output, energy_score = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m loss = criterion(output, target)\n\u001b[32m    205\u001b[39m mean_v = model.probes.mean(dim=\u001b[32m0\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m).view(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, K_PROBES, K_PROBES)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1783\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1794\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1790\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1792\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1793\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1797\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mMNISTClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    121\u001b[39m     x.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Compute derivatives with respect to input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m O0 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_local_fisher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# For image tensors, use small Gaussian perturbations instead of probe directions\u001b[39;00m\n\u001b[32m    127\u001b[39m mean_v = \u001b[38;5;28mself\u001b[39m.probes.mean(dim=\u001b[32m0\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m).view(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.k_probes, \u001b[38;5;28mself\u001b[39m.k_probes)  \u001b[38;5;66;03m# [1, 1, 28, 28]\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mMNISTClassifier.compute_local_fisher\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_local_fisher\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     fisher_info = (scores ** \u001b[32m2\u001b[39m).mean(dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fisher_info\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mMNISTClassifier.compute_score\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    106\u001b[39m probes_flat = \u001b[38;5;28mself\u001b[39m.probes.view(\u001b[38;5;28mself\u001b[39m.k_probes, -\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [k_probes, 28*28]\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Project gradients onto probe directions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m scores = -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobes_flat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, k_probes]\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (256x150528 and 784x28)"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch, torchvision, transformers\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "LR = 0.001\n",
    "\n",
    "K_PROBES = 28  ## seems to match with input size\n",
    "EPSILON = 0.1\n",
    "LAMBDA_JET = 0.1      # Weight for alignment in Jet Loss\n",
    "ETA_JET = 0.5         # Weight of Jet Loss in total loss [cite: 82]\n",
    "\n",
    "# Device setup for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, k_probes, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.k_probes = k_probes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Probes should match input dimensions: 3 channels × 224 × 224 = 150528\n",
    "        v = torch.randn(self.k_probes, self.k_probes*self.k_probes)  # Probe directions for RGB 224×224 images\n",
    "        v = v / torch.norm(v, dim=1, keepdim=True)\n",
    "        self.register_buffer('probes', v)\n",
    "\n",
    "        # ViT backbone\n",
    "        vitmodel = torchvision.models.vit_b_16(pretrained=False)\n",
    "        \n",
    "        # Store components needed for proper ViT preprocessing\n",
    "        self.conv_proj = vitmodel.conv_proj\n",
    "        self.class_token = vitmodel.class_token\n",
    "        self.encoder_pos_embedding = vitmodel.encoder.pos_embedding\n",
    "        self.encoder_dropout = vitmodel.encoder.dropout\n",
    "        \n",
    "        self.model_upper = nn.Sequential(\n",
    "            vitmodel.encoder.layers[0],\n",
    "            vitmodel.encoder.layers[1],\n",
    "            vitmodel.encoder.layers[2],\n",
    "            vitmodel.encoder.layers[3],\n",
    "            vitmodel.encoder.layers[4],\n",
    "            vitmodel.encoder.layers[5],\n",
    "        )\n",
    "\n",
    "        self.scalar_projection = nn.Sequential(\n",
    "            vitmodel.encoder.ln,\n",
    "            nn.Linear(in_features=768, out_features=1, bias=True),\n",
    "        )\n",
    "\n",
    "        self.model_lower = nn.Sequential(\n",
    "            nn.LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
    "            vitmodel.encoder.layers[6],\n",
    "            vitmodel.encoder.layers[7],\n",
    "            vitmodel.encoder.layers[8],\n",
    "            vitmodel.encoder.layers[9],\n",
    "            vitmodel.encoder.layers[10],\n",
    "            vitmodel.encoder.layers[11],\n",
    "            vitmodel.encoder.ln,\n",
    "            nn.Linear(in_features=768, out_features=10, bias=True),\n",
    "        )\n",
    "        \n",
    "    def compute_score(self, x):\n",
    "        if not x.requires_grad:\n",
    "            x.requires_grad_(True)\n",
    "\n",
    "        # Proper ViT preprocessing\n",
    "        # 1. Apply conv projection: [B, 3, 224, 224] -> [B, 768, 14, 14]\n",
    "        x_proj = self.conv_proj(x)\n",
    "        x_proj = self.encoder_dropout(x_proj)\n",
    "        h = self.model_upper(x_proj)\n",
    "        energy = self.scalar_projection(h)\n",
    "\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=energy.sum(),\n",
    "            inputs=x,\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        grads_flat = grads.view(grads.size(0), -1)  # [batch, 3*224*224]\n",
    "        probes_flat = self.probes.view(self.k_probes, -1)  # [k_probes, 3*224*224]\n",
    "\n",
    "        # Project gradients onto probe directions\n",
    "        scores = -torch.matmul(grads_flat, probes_flat.T)  # [batch, k_probes]\n",
    "        return scores\n",
    "\n",
    "    def compute_local_fisher(self, x):\n",
    "        scores = self.compute_score(x)\n",
    "        fisher_info = (scores ** 2).mean(dim=1, keepdim=True)\n",
    "        return fisher_info\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Data is already 3-channel from transform\n",
    "        with torch.set_grad_enabled(True):\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad_(True)\n",
    "\n",
    "            # Compute derivatives with respect to input\n",
    "            O0 = self.compute_local_fisher(x)\n",
    "\n",
    "            # For image tensors, use small Gaussian perturbations\n",
    "            # Mean of probes has shape [3*224*224], reshape to [3, 224, 224]\n",
    "            mean_v = self.probes.mean(dim=0, keepdim=True).view(1, 3, 224, 224)\n",
    "            x_pos = x + mean_v\n",
    "            x_neg = x - mean_v\n",
    "\n",
    "            I_pos = self.compute_local_fisher(x_pos)\n",
    "            I_neg = self.compute_local_fisher(x_neg)\n",
    "\n",
    "            O1 = (I_pos - I_neg) / (2 * self.epsilon)\n",
    "            O2 = (I_pos - 2 * O0 + I_neg) / (self.epsilon ** 2)\n",
    "\n",
    "        # Forward through ViT preprocessing\n",
    "        # 1. Apply conv projection\n",
    "        x_proj = self.conv_proj(x)\n",
    "        \n",
    "        # 2. Reshape: [B, 768, 14, 14] -> [B, 196, 768]\n",
    "        batch_size = x_proj.shape[0]\n",
    "        x_proj = x_proj.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # 3. Add class token\n",
    "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
    "        x_proj = torch.cat([class_token, x_proj], dim=1)\n",
    "        \n",
    "        # 4. Add positional embeddings and apply dropout\n",
    "        x_proj = x_proj + self.encoder_pos_embedding\n",
    "        x_proj = self.encoder_dropout(x_proj)\n",
    "        \n",
    "        # 5. Pass through encoder layers\n",
    "        h = self.model_upper(x_proj)  # [batch, 197, 768]\n",
    "\n",
    "        # Concatenate geometric features with representation\n",
    "        # Reshape O0, O1, O2 to match: [batch, 1] -> [batch, 1, 768]\n",
    "        O0_expanded = O0.unsqueeze(-1).expand(-1, -1, 768)\n",
    "        O1_expanded = O1.unsqueeze(-1).expand(-1, -1, 768)\n",
    "        O2_expanded = O2.unsqueeze(-1).expand(-1, -1, 768)\n",
    "        \n",
    "        features = torch.cat([h, O0_expanded, O1_expanded, O2_expanded], dim=1)  # [batch, 200, 768]\n",
    "\n",
    "        y_hat = self.model_lower(features)\n",
    "\n",
    "        return y_hat, O0 # Return O0 for viz/loss if needed\n",
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel RGB\n",
    "    # transforms.Resize((224, 224)),  # Resize to ViT input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081))  # MNIST mean and std for 3 channels\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create model\n",
    "model = MNISTClassifier(NUM_CLASSES, K_PROBES, EPSILON)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Starting MNIST Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, energy_score = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        mean_v = model.probes.mean(dim=0, keepdim=True).view(1, 1, K_PROBES, K_PROBES)\n",
    "\n",
    "        # (Uncomment below to enable Jet Loss - adds compute time)\n",
    "        x_pos = data + EPSILON * mean_v\n",
    "        x_neg = data - EPSILON * mean_v\n",
    "        pred_pos, _ = model(x_pos)\n",
    "        pred_neg, _ = model(x_neg)\n",
    "        D_hat = (pred_pos - pred_neg) / (2 * EPSILON) # [cite: 75]\n",
    "        score_proj = model.compute_score(data).mean(dim=1, keepdim=True)  # [batch, 1]\n",
    "        loss_jet = ((D_hat + LAMBDA_JET * score_proj)**2).mean() #\n",
    "        loss += ETA_JET * loss_jet\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}: Loss={loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    test_accuracies.append(accuracy)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {avg_loss:.4f} | Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot training results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('MNIST Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', color='green')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('MNIST Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('mnist_training.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "print(\"Training plots saved as 'mnist_training.png'\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec549b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab11cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training ViT_b_16 on MNIST Dataset\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: ViT_b_16\n",
      "Device: cuda\n",
      "Training samples: 60000, Test samples: 10000\n",
      "\n",
      "Starting training...\n",
      "Epoch [1/10], Batch [0/938], Loss: 2.4713\n",
      "Epoch [1/10], Batch [100/938], Loss: 2.2972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m loss.backward()\n\u001b[32m     52\u001b[39m vit_optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m epoch_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m num_batches += \u001b[32m1\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "LR = 0.001\n",
    "\n",
    "# Create data loaders with batch size 256 for ViT training\n",
    "vit_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "vit_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load ViT_b_16 model from torchvision\n",
    "vit_model = torchvision.models.vit_b_16(pretrained=False)\n",
    "\n",
    "# Modify the head for MNIST (10 classes)\n",
    "vit_model.heads = nn.Linear(vit_model.heads.head.in_features, 10)\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "# Setup optimizer and loss\n",
    "vit_optimizer = optim.Adam(vit_model.parameters(), lr=LR)\n",
    "vit_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Lists to store metrics\n",
    "vit_train_losses = []\n",
    "vit_test_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    vit_model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(vit_train_loader):\n",
    "        # Move data to device (data is already [batch, 3, 224, 224] from transform)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        vit_optimizer.zero_grad()\n",
    "        output = vit_model(data)\n",
    "        loss = vit_criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        vit_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if batch_idx % PRINT_FREQ == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{batch_idx}/{len(vit_train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    vit_train_losses.append(avg_loss)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    vit_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in vit_test_loader:\n",
    "            # Move data to device (data is already [batch, 3, 224, 224] from transform)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = vit_model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    vit_test_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_loss:.4f} | Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final Test Accuracy: {vit_test_accuracies[-1]:.2f}%\")\n",
    "print(f\"Best Test Accuracy: {max(vit_test_accuracies):.2f}%\")\n",
    "\n",
    "# Plot training results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# Plot training loss\n",
    "# Plot training loss\n",
    "ax1.plot(range(1, EPOCHS+1), vit_train_losses, marker='o', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('ViT_b_16 Training Loss on MNIST', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(range(1,     EPOCHS+1))\n",
    "\n",
    "# Plot test accuracy\n",
    "ax2.plot(range(1, EPOCHS+1), vit_test_accuracies, marker='s', color='green', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('ViT_b_16 Test Accuracy on MNIST', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(range(1, EPOCHS+1))\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nTraining summary saved to plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d098bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "LR = 0.001\n",
    "\n",
    "K_PROBES = 16  ## seems to match with input size\n",
    "EPSILON = 0.1\n",
    "LAMBDA_JET = 0.1      # Weight for alignment in Jet Loss\n",
    "ETA_JET = 0.5         # Weight of Jet Loss in total loss [cite: 82]\n",
    "\n",
    "# Device setup for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, k_probes, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.k_probes = k_probes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        v = torch.randn(k_probes, k_probes*k_probes)  # 2D probe directions\n",
    "        v = v / torch.norm(v, dim=1, keepdim=True)\n",
    "        self.register_buffer('probes', v)\n",
    "\n",
    "        # ViT backbone\n",
    "        vitmodel = torchvision.models.vit_b_16(pretrained=False)\n",
    "        self.model_upper = nn.Sequential(\n",
    "            vitmodel.conv_proj,\n",
    "            vitmodel.encoder.dropout,\n",
    "            vitmodel.encoder.dropout,\n",
    "            vitmodel.encoder.layers[0],\n",
    "            vitmodel.encoder.layers[1],\n",
    "            vitmodel.encoder.layers[2],\n",
    "            vitmodel.encoder.layers[3],\n",
    "        )\n",
    "        self.model_middle = nn.Sequential(\n",
    "            vitmodel.encoder.layers[4],\n",
    "            vitmodel.encoder.layers[5],\n",
    "            vitmodel.encoder.layers[6],\n",
    "            vitmodel.encoder.layers[7],\n",
    "        )\n",
    "\n",
    "        self.scalar_projection = nn.Sequential(\n",
    "            vitmodel.encoder.ln,\n",
    "            nn.Linear(in_features=768, out_features=1, bias=True),\n",
    "        )\n",
    "\n",
    "        self.model_lower = nn.Sequential(\n",
    "            vitmodel.encoder.layers[8],\n",
    "            vitmodel.encoder.layers[9],\n",
    "            vitmodel.encoder.layers[10],\n",
    "            vitmodel.encoder.layers[11],\n",
    "            vitmodel.encoder.ln,\n",
    "            nn.Linear(in_features=768, out_features=10, bias=True),\n",
    "        )\n",
    "        \n",
    "    def compute_score(self, x):\n",
    "        if not x.requires_grad:\n",
    "            x.requires_grad_(True)\n",
    "\n",
    "        # Compute energy as mean of representation\n",
    "        h = self.model_upper(x)\n",
    "        energy = self.scalar_projection(h)\n",
    "\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=energy.sum(),\n",
    "            inputs=x,\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        grads_flat = grads.view(grads.size(0), -1)  # [batch, 784]\n",
    "        probes_flat = self.probes.view(self.k_probes, -1)  # [k_probes, 784]\n",
    "\n",
    "        # Use only first 2 dimensions or project to 2D space\n",
    "        scores = -torch.matmul(grads_flat, probes_flat.T)  # [batch, k_probes]\n",
    "        return scores\n",
    "\n",
    "    def compute_local_fisher(self, x):\n",
    "        scores = self.compute_score(x)\n",
    "        fisher_info = (scores ** 2).mean(dim=1, keepdim=True)\n",
    "        return fisher_info\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.set_grad_enabled(True):\n",
    "            if not x.requires_grad:\n",
    "                x.requires_grad_(True)\n",
    "\n",
    "            # Compute derivatives with respect to input\n",
    "            O0 = self.compute_local_fisher(x)\n",
    "\n",
    "            # For image tensors, use small Gaussian perturbations instead of probe directions\n",
    "            mean_v = self.probes.mean(dim=0, keepdim=True).view(1, 1, self.k_probes, self.k_probes)  # [1, 1, 16, 16]\n",
    "            x_pos = x + mean_v\n",
    "            x_neg = x - mean_v\n",
    "\n",
    "            I_pos = self.compute_local_fisher(x_pos)\n",
    "            I_neg = self.compute_local_fisher(x_neg)\n",
    "\n",
    "            O1 = (I_pos - I_neg) / (2 * self.epsilon)\n",
    "            O2 = (I_pos - 2 * O0 + I_neg) / (self.epsilon ** 2)\n",
    "\n",
    "        # Forward through ViT preprocessing\n",
    "        h = self.model_upper(x)  # [batch, 768, 14, 14]\n",
    "\n",
    "        # Concatenate all features\n",
    "        features = torch.cat([h, O0, O1, O2], dim=1)  # [batch, 131, 4, 4]\n",
    "\n",
    "        y_hat = self.model_lower(features)\n",
    "\n",
    "        return y_hat, O0 # Return O0 for viz/loss if needed\n",
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel RGB\n",
    "    transforms.Resize((224, 224)),  # Resize to ViT input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081))  # MNIST mean and std for 3 channels\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create model\n",
    "model = MNISTClassifier(NUM_CLASSES, K_PROBES, EPSILON)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Starting MNIST Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, energy_score = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        mean_v = model.probes.mean(dim=0, keepdim=True).view(1, 1, K_PROBES, K_PROBES)\n",
    "\n",
    "        # (Uncomment below to enable Jet Loss - adds compute time)\n",
    "        x_pos = data + EPSILON * mean_v\n",
    "        x_neg = data - EPSILON * mean_v\n",
    "        pred_pos, _ = model(x_pos)\n",
    "        pred_neg, _ = model(x_neg)\n",
    "        D_hat = (pred_pos - pred_neg) / (2 * EPSILON) # [cite: 75]\n",
    "        score_proj = model.compute_score(data).mean(dim=1, keepdim=True)  # [batch, 1]\n",
    "        loss_jet = ((D_hat + LAMBDA_JET * score_proj)**2).mean() #\n",
    "        loss += ETA_JET * loss_jet\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}: Loss={loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    test_accuracies.append(accuracy)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {avg_loss:.4f} | Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot training results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('MNIST Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', color='green')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('MNIST Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('mnist_training.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "print(\"Training plots saved as 'mnist_training.png'\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
