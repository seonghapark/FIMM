{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78899a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch, torchvision, transformers\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688dcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Train samples: 60000, Test samples: 10000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "VisionTransformer.__init__() got an unexpected keyword argument 'pretraine'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 252\u001b[39m\n\u001b[32m    249\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m logits\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m model = \u001b[43mMNISTClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_PROBES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPSILON\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m model = model.to(device)\n\u001b[32m    255\u001b[39m optimizer = optim.Adam(model.parameters(), lr=LR)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mMNISTClassifier.__init__\u001b[39m\u001b[34m(self, num_classes, k_probes, epsilon)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mself\u001b[39m.register_buffer(\u001b[33m'\u001b[39m\u001b[33mprobes\u001b[39m\u001b[33m'\u001b[39m, v)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# ViT backbone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m vith14 = \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvit_h_14\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretraine\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m vitmodel = torchvision.models.vit_b_16(pretrained=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.conv_proj = vitmodel.conv_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:142\u001b[39m, in \u001b[36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m     warnings.warn(\n\u001b[32m    136\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs.keys()),\u001b[38;5;250m \u001b[39mseparate_last=\u001b[33m'\u001b[39m\u001b[33mand \u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as positional \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m     kwargs.update(keyword_only_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:228\u001b[39m, in \u001b[36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[32m    226\u001b[39m     kwargs[weights_param] = default_weights_arg\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torchvision/models/vision_transformer.py:777\u001b[39m, in \u001b[36mvit_h_14\u001b[39m\u001b[34m(weights, progress, **kwargs)\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    759\u001b[39m \u001b[33;03mConstructs a vit_h_14 architecture from\u001b[39;00m\n\u001b[32m    760\u001b[39m \u001b[33;03m`An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    773\u001b[39m \u001b[33;03m    :members:\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    775\u001b[39m weights = ViT_H_14_Weights.verify(weights)\n\u001b[32m--> \u001b[39m\u001b[32m777\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vision_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torchvision/models/vision_transformer.py:324\u001b[39m, in \u001b[36m_vision_transformer\u001b[39m\u001b[34m(patch_size, num_layers, num_heads, hidden_dim, mlp_dim, weights, progress, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m     _ovewrite_named_param(kwargs, \u001b[33m\"\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m\"\u001b[39m, weights.meta[\u001b[33m\"\u001b[39m\u001b[33mmin_size\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m    322\u001b[39m image_size = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m224\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m model = \u001b[43mVisionTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights:\n\u001b[32m    335\u001b[39m     model.load_state_dict(weights.get_state_dict(progress=progress, check_hash=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[31mTypeError\u001b[39m: VisionTransformer.__init__() got an unexpected keyword argument 'pretraine'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "LR = 0.001\n",
    "\n",
    "K_PROBES = 4\n",
    "EPSILON = 0.1\n",
    "\n",
    "# Device setup for CUDA\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, k_probes, epsilon):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.k_probes = k_probes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        v = torch.randn(k_probes, 2)  # 2D probe directions\n",
    "        v = v / torch.norm(v, dim=1, keepdim=True)\n",
    "        self.register_buffer('probes', v)\n",
    "\n",
    "        # ViT backbone\n",
    "        vitmodel = torchvision.models.vit_b_16(pretrained=False)\n",
    "        self.conv_proj = vitmodel.conv_proj\n",
    "        \n",
    "        # Get the hidden dimension and sequence length\n",
    "        self.hidden_dim = vitmodel.hidden_dim\n",
    "        self.seq_length = (224 // 16) ** 2  # For 224x224 images with patch size 16\n",
    "        \n",
    "        # Class token and position embeddings\n",
    "        self.class_token = vitmodel.class_token\n",
    "        self.encoder_pos_embedding = vitmodel.encoder.pos_embedding\n",
    "        self.encoder_dropout = vitmodel.encoder.dropout\n",
    "        \n",
    "        self.layer0 = vitmodel.encoder.layers[0]\n",
    "        self.layer1 = vitmodel.encoder.layers[1]\n",
    "        self.layer2 = vitmodel.encoder.layers[2]\n",
    "        self.layer3 = vitmodel.encoder.layers[3]\n",
    "        # self.layer4 = vitmodel.encoder.layers[4]\n",
    "        # self.layer5 = vitmodel.encoder.layers[5]\n",
    "        # self.layer6 = vitmodel.encoder.layers[6]\n",
    "        # self.layer7 = vitmodel.encoder.layers[7]\n",
    "        # self.layer8 = vitmodel.encoder.layers[8]\n",
    "        # self.layer9 = vitmodel.encoder.layers[9]\n",
    "        # self.layer10 = vitmodel.encoder.layers[10]\n",
    "        # self.layer11 = vitmodel.encoder.layers[11]\n",
    "        self.encoder_ln = vitmodel.encoder.ln\n",
    "        \n",
    "        # Classifier with Fisher features\n",
    "        self.classifier = nn.Linear(in_features=768+3+3, out_features=10, bias=True)\n",
    "        \n",
    "        # # Calculate feature dimension: 12 layers * 1 fisher dim + 3*1 (O0,O1,O2) = 15\n",
    "        # fisher_dim = 12  # 12 encoder layers\n",
    "        # self.feature_dim = fisher_dim + 3  # Plus O0, O1, O2\n",
    "        \n",
    "        # # Classifier layer\n",
    "        # self.classifier = nn.Linear(in_features=self.feature_dim, out_features=NUM_CLASSES, bias=True)\n",
    "\n",
    "    def compute_score(self, x):\n",
    "        if not x.requires_grad:\n",
    "            x.requires_grad_(True)\n",
    "\n",
    "        # Compute energy as mean of representation\n",
    "        energy = x.mean()\n",
    "\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=energy,\n",
    "            inputs=x,\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        grads_flat = grads.view(grads.size(0), -1)\n",
    "\n",
    "        # Use only first 2 dimensions or project to 2D space\n",
    "        if grads_flat.size(1) >= 2:\n",
    "            grads_2d = grads_flat[:, :2]\n",
    "        else:\n",
    "            grads_2d = grads_flat\n",
    "\n",
    "        scores = -torch.matmul(grads_2d, self.probes.T)\n",
    "        return scores\n",
    "\n",
    "    def compute_local_fisher(self, x):\n",
    "        scores = self.compute_score(x)\n",
    "        fisher_info = (scores ** 2).mean(dim=1, keepdim=True)\n",
    "        return fisher_info\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, 1, 28, 28]\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Convert 1-channel to 3-channel (RGB) by repeating\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)  # [batch, 3, 28, 28]\n",
    "\n",
    "        # Resize to ViT input size (224x224)\n",
    "        x_resized = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            if not x_resized.requires_grad:\n",
    "                x_resized.requires_grad_(True)\n",
    "\n",
    "            # Compute derivatives with respect to input\n",
    "            O0 = self.compute_local_fisher(x_resized)\n",
    "\n",
    "            # For image tensors, use small Gaussian perturbations instead of probe directions\n",
    "            perturbation = torch.randn_like(x_resized) * self.epsilon\n",
    "            x_pos = x_resized + perturbation\n",
    "            x_neg = x_resized - perturbation\n",
    "\n",
    "            I_pos = self.compute_local_fisher(x_pos)\n",
    "            I_neg = self.compute_local_fisher(x_neg)\n",
    "\n",
    "            O1 = (I_pos - I_neg) / (2 * self.epsilon)\n",
    "            O2 = (I_pos - 2 * O0 + I_neg) / (self.epsilon ** 2)\n",
    "\n",
    "        # Forward through ViT preprocessing\n",
    "        # Conv projection\n",
    "        h = self.conv_proj(x_resized)  # [batch, 768, 14, 14]\n",
    "        \n",
    "        # Reshape from [batch, 768, 14, 14] to [batch, 196, 768]\n",
    "        h = h.flatten(2).transpose(1, 2)  # [batch, 196, 768]\n",
    "        \n",
    "        # Add class token\n",
    "        batch_class_token = self.class_token.expand(batch_size, -1, -1)  # [batch, 1, 768]\n",
    "        h = torch.cat([batch_class_token, h], dim=1)  # [batch, 197, 768]\n",
    "        \n",
    "        # Add position embeddings\n",
    "        h = h + self.encoder_pos_embedding\n",
    "        \n",
    "        # Apply dropout\n",
    "        h = self.encoder_dropout(h)\n",
    "\n",
    "        # Apply layers and compute Fisher at each stage\n",
    "        h = self.layer0(h)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            if not h.requires_grad:\n",
    "                h.requires_grad_(True)\n",
    "            fisher_layer0 = self.compute_local_fisher(h)\n",
    "\n",
    "        h = self.layer1(h)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            if not h.requires_grad:\n",
    "                h.requires_grad_(True)\n",
    "            fisher_layer1 = self.compute_local_fisher(h)\n",
    "\n",
    "        h = self.layer2(h)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            if not h.requires_grad:\n",
    "                h.requires_grad_(True)\n",
    "            fisher_layer2 = self.compute_local_fisher(h)\n",
    "\n",
    "        h = self.layer3(h)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            if not h.requires_grad:\n",
    "                h.requires_grad_(True)\n",
    "        #     fisher_layer3 = self.compute_local_fisher(h)\n",
    "\n",
    "        # h = self.layer4(h)\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        #     if not h.requires_grad:\n",
    "        #         h.requires_grad_(True)\n",
    "        #     fisher_layer4 = self.compute_local_fisher(h)\n",
    "\n",
    "        # h = self.layer5(h)\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        #     if not h.requires_grad:\n",
    "        #         h.requires_grad_(True)\n",
    "        #     fisher_layer5 = self.compute_local_fisher(h)\n",
    "\n",
    "        # h = self.layer6(h)\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        #     if not h.requires_grad:\n",
    "        #         h.requires_grad_(True)\n",
    "        #     fisher_layer6 = self.compute_local_fisher(h)\n",
    "\n",
    "        # h = self.layer7(h)\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        #     if not h.requires_grad:\n",
    "        #         h.requires_grad_(True)\n",
    "        #     fisher_layer7 = self.compute_local_fisher(h)\n",
    "\n",
    "        # h = self.layer8(h)\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        #     if not h.requires_grad:\n",
    "        #         h.requires_grad_(True)\n",
    "        #     fisher_layer8 = self.compute_local_fisher(h)\n",
    "\n",
    "        # h = self.layer9(h)\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        #     if not h.requires_grad:\n",
    "        #         h.requires_grad_(True)\n",
    "        #     fisher_layer9 = self.compute_local_fisher(h)\n",
    "\n",
    "        # h = self.layer10(h)\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        #     if not h.requires_grad:\n",
    "        #         h.requires_grad_(True)\n",
    "        #     fisher_layer10 = self.compute_local_fisher(h)\n",
    "\n",
    "        # h = self.layer11(h)\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        #     if not h.requires_grad:\n",
    "        #         h.requires_grad_(True)\n",
    "        #     # fisher_layer11 = self.compute_local_fisher(h)\n",
    "\n",
    "        # Apply layer normalization\n",
    "        h = self.encoder_ln(h)\n",
    "        \n",
    "        # Extract class token representation (first token)\n",
    "        h_cls = h[:, 0]  # [batch, 768]\n",
    "\n",
    "        # Concatenate all Fisher features (each [batch, 1])\n",
    "        fisher_concat = torch.cat([\n",
    "            fisher_layer0, fisher_layer1, fisher_layer2         ###, fisher_layer3, fisher_layer4, \n",
    "            # fisher_layer5, fisher_layer6, fisher_layer7, fisher_layer8, fisher_layer9, \n",
    "            # fisher_layer10, fisher_layer11\n",
    "        ], dim=1)  # [batch, 12]\n",
    "\n",
    "        # Combine all features: h_cls [batch, 768] + O0, O1, O2 [batch, 3] + fisher_concat [batch, 11] = [batch, 782]\n",
    "        # Combine all features: h_cls [batch, 768] + O0, O1, O2 [batch, 3] + fisher_concat [batch, 3] = [batch, 774]\n",
    "        features = torch.cat([h_cls, O0, O1, O2, fisher_concat], dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "model = MNISTClassifier(NUM_CLASSES, K_PROBES, EPSILON)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Starting MNIST Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}: Loss={loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    test_accuracies.append(accuracy)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {avg_loss:.4f} | Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "\n",
    "# Plot training results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('MNIST Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', color='green')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('MNIST Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('mnist_training.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "print(\"Training plots saved as 'mnist_training.png'\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec549b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab11cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training ViT_b_16 on MNIST Dataset\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: ViT_b_16\n",
      "Device: cuda\n",
      "Training samples: 60000, Test samples: 10000\n",
      "\n",
      "Starting training...\n",
      "Epoch [1/10], Batch [0/938], Loss: 2.4713\n",
      "Epoch [1/10], Batch [100/938], Loss: 2.2972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m loss.backward()\n\u001b[32m     52\u001b[39m vit_optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m epoch_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m num_batches += \u001b[32m1\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train ViT_b_16 model on MNIST\n",
    "print(\"=\" * 60)\n",
    "print(\"Training ViT_b_16 on MNIST Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create data loaders with batch size 256 for ViT training\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 30\n",
    "PRINT_FREQ = 100\n",
    "VIT_BATCH_SIZE = 512\n",
    "\n",
    "vit_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=VIT_BATCH_SIZE, shuffle=True)\n",
    "vit_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=VIT_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load ViT_b_16 model from torchvision\n",
    "vit_model_origin = torchvision.models.vit_b_16(pretrained=False)\n",
    "\n",
    "vit_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16)),\n",
    "    nn.Dropout(p=0.0, inplace=False),\n",
    "    vit_model_origin.encoder.layers[0],\n",
    "    vit_model_origin.encoder.layers[1],\n",
    "    vit_model_origin.encoder.layers[2],\n",
    "    vit_model_origin.encoder.layers[3],\n",
    "    vit_model_origin.encoder.ln,\n",
    "    nn.Linear(in_features=768, out_features=10, bias=True)\n",
    ")\n",
    "\n",
    "# Modify the head for MNIST (10 classes)\n",
    "# vit_model.heads = nn.Linear(vit_model.heads.head.in_features, 10)\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "print(f\"Model loaded: ViT_b_16\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch size: {VIT_BATCH_SIZE}\")\n",
    "print(f\"Training samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Setup optimizer and loss\n",
    "vit_optimizer = optim.Adam(vit_model.parameters(), lr=0.001)\n",
    "vit_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Lists to store metrics\n",
    "vit_train_losses = []\n",
    "vit_test_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    vit_model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(vit_train_loader):\n",
    "        # Move data to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Convert 1-channel to 3-channel (RGB) and resize to 224x224\n",
    "        data = data.repeat(1, 3, 1, 1)  # [batch, 3, 28, 28]\n",
    "        data = F.interpolate(data, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Forward pass\n",
    "        vit_optimizer.zero_grad()\n",
    "        output = vit_model(data)\n",
    "        loss = vit_criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        vit_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if batch_idx % PRINT_FREQ == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx}/{len(vit_train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    vit_train_losses.append(avg_loss)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    vit_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in vit_test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Convert 1-channel to 3-channel and resize\n",
    "            data = data.repeat(1, 3, 1, 1)\n",
    "            data = F.interpolate(data, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = vit_model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    vit_test_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {avg_loss:.4f} | Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final Test Accuracy: {vit_test_accuracies[-1]:.2f}%\")\n",
    "print(f\"Best Test Accuracy: {max(vit_test_accuracies):.2f}%\")\n",
    "\n",
    "# Plot training results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot training loss\n",
    "ax1.plot(range(1, NUM_EPOCHS+1), vit_train_losses, marker='o', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('ViT_b_16 Training Loss on MNIST', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(range(1, NUM_EPOCHS+1))\n",
    "\n",
    "# Plot test accuracy\n",
    "ax2.plot(range(1, NUM_EPOCHS+1), vit_test_accuracies, marker='s', color='green', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('ViT_b_16 Test Accuracy on MNIST', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(range(1, NUM_EPOCHS+1))\n",
    "ax2.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining summary saved to plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d098bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=False)\n",
      "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (4): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vit_model_origin = torchvision.models.vit_b_16(pretrained=False)\n",
    "print(vit_model_origin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderBlock(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (mlp): MLPBlock(\n",
      "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (4): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\n",
    "    vit_model_origin.encoder.layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd5ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
