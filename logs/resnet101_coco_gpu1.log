NOTE! Installing ujson may make loading annotations faster.
Using device: cuda:1
Loading COCO dataset...
Loading annotations into memory...
Done (t=8.82s)
Creating index...
index created!
Loading annotations into memory...
Done (t=0.37s)
Creating index...
index created!
Train samples: 117266, Val samples: 4952
Number of classes in mapping: 80

--- Baseline ResNet101 ---
Epoch 0, Batch 0/3665: Loss=4.5562
Epoch 0, Batch 150/3665: Loss=3.6300
Epoch 0, Batch 300/3665: Loss=2.9932
Epoch 0, Batch 450/3665: Loss=3.3769
Epoch 0, Batch 600/3665: Loss=3.2953
Epoch 0, Batch 750/3665: Loss=3.1659
Epoch 0, Batch 900/3665: Loss=3.1565
Epoch 0, Batch 1050/3665: Loss=3.6815
Epoch 0, Batch 1200/3665: Loss=3.2516
Epoch 0, Batch 1350/3665: Loss=3.4757
Epoch 0, Batch 1500/3665: Loss=3.6769
Epoch 0, Batch 1650/3665: Loss=3.1813
Epoch 0, Batch 1800/3665: Loss=3.5128
Epoch 0, Batch 1950/3665: Loss=3.6283
Epoch 0, Batch 2100/3665: Loss=3.4171
Epoch 0, Batch 2250/3665: Loss=2.9613
Epoch 0, Batch 2400/3665: Loss=2.4896
Epoch 0, Batch 2550/3665: Loss=3.1448
Epoch 0, Batch 2700/3665: Loss=3.3258
Epoch 0, Batch 2850/3665: Loss=3.2076
Epoch 0, Batch 3000/3665: Loss=3.2988
Epoch 0, Batch 3150/3665: Loss=3.0405
Epoch 0, Batch 3300/3665: Loss=2.7113
Epoch 0, Batch 3450/3665: Loss=3.1887
Epoch 0, Batch 3600/3665: Loss=3.0594
Epoch 0 | Train Loss: 3.2608 | Val Accuracy: 22.50%
Epoch 1, Batch 0/3665: Loss=2.9819
Epoch 1, Batch 150/3665: Loss=3.0674
Epoch 1, Batch 300/3665: Loss=2.7693
Epoch 1, Batch 450/3665: Loss=3.1448
Epoch 1, Batch 600/3665: Loss=2.4802
Epoch 1, Batch 750/3665: Loss=3.1779
Epoch 1, Batch 900/3665: Loss=3.1625
Epoch 1, Batch 1050/3665: Loss=3.1148
Epoch 1, Batch 1200/3665: Loss=2.4268
Epoch 1, Batch 1350/3665: Loss=3.0045
Epoch 1, Batch 1500/3665: Loss=2.4963
Epoch 1, Batch 1650/3665: Loss=2.9253
Epoch 1, Batch 1800/3665: Loss=3.0868
Epoch 1, Batch 1950/3665: Loss=2.5125
Epoch 1, Batch 2100/3665: Loss=2.6180
Epoch 1, Batch 2250/3665: Loss=2.8716
Epoch 1, Batch 2400/3665: Loss=2.9220
Epoch 1, Batch 2550/3665: Loss=2.6068
Epoch 1, Batch 2700/3665: Loss=2.9857
Epoch 1, Batch 2850/3665: Loss=3.0546
Epoch 1, Batch 3000/3665: Loss=3.4501
Epoch 1, Batch 3150/3665: Loss=2.8158
Epoch 1, Batch 3300/3665: Loss=2.5466
Epoch 1, Batch 3450/3665: Loss=2.5538
Epoch 1, Batch 3600/3665: Loss=2.9200
Epoch 1 | Train Loss: 2.9070 | Val Accuracy: 28.66%
Epoch 2, Batch 0/3665: Loss=2.7754
Epoch 2, Batch 150/3665: Loss=2.5229
Epoch 2, Batch 300/3665: Loss=2.4079
Epoch 2, Batch 450/3665: Loss=3.1280
Epoch 2, Batch 600/3665: Loss=2.3808
Epoch 2, Batch 750/3665: Loss=3.3570
Epoch 2, Batch 900/3665: Loss=3.1075
Epoch 2, Batch 1050/3665: Loss=2.3417
Epoch 2, Batch 1200/3665: Loss=2.5370
Epoch 2, Batch 1350/3665: Loss=3.3743
Epoch 2, Batch 1500/3665: Loss=2.9720
Epoch 2, Batch 1650/3665: Loss=2.7758
Epoch 2, Batch 1800/3665: Loss=2.7942
Epoch 2, Batch 1950/3665: Loss=2.7376
Epoch 2, Batch 2100/3665: Loss=2.9444
Epoch 2, Batch 2250/3665: Loss=2.8399
Epoch 2, Batch 2400/3665: Loss=2.4012
Epoch 2, Batch 2550/3665: Loss=2.5858
Epoch 2, Batch 2700/3665: Loss=3.1147
Epoch 2, Batch 2850/3665: Loss=2.6771
Epoch 2, Batch 3000/3665: Loss=2.6183
Epoch 2, Batch 3150/3665: Loss=2.6392
Epoch 2, Batch 3300/3665: Loss=2.5627
Epoch 2, Batch 3450/3665: Loss=2.3650
Epoch 2, Batch 3600/3665: Loss=2.3094
Epoch 2 | Train Loss: 2.7210 | Val Accuracy: 31.02%
Epoch 3, Batch 0/3665: Loss=2.4986
Epoch 3, Batch 150/3665: Loss=2.4137
Epoch 3, Batch 300/3665: Loss=2.6820
Epoch 3, Batch 450/3665: Loss=2.5056
Epoch 3, Batch 600/3665: Loss=2.4233
Epoch 3, Batch 750/3665: Loss=2.8361
Epoch 3, Batch 900/3665: Loss=2.4532
Epoch 3, Batch 1050/3665: Loss=2.5988
Epoch 3, Batch 1200/3665: Loss=2.1802
Epoch 3, Batch 1350/3665: Loss=2.3106
Epoch 3, Batch 1500/3665: Loss=2.8313
Epoch 3, Batch 1650/3665: Loss=2.9692
Epoch 3, Batch 1800/3665: Loss=1.8979
Epoch 3, Batch 1950/3665: Loss=2.5744
Epoch 3, Batch 2100/3665: Loss=2.5841
Epoch 3, Batch 2250/3665: Loss=2.3862
Epoch 3, Batch 2400/3665: Loss=2.6976
Epoch 3, Batch 2550/3665: Loss=2.6810
Epoch 3, Batch 2700/3665: Loss=2.7226
Epoch 3, Batch 2850/3665: Loss=2.5536
Epoch 3, Batch 3000/3665: Loss=2.9066
Epoch 3, Batch 3150/3665: Loss=2.1295
Epoch 3, Batch 3300/3665: Loss=2.2668
Epoch 3, Batch 3450/3665: Loss=2.6326
Epoch 3, Batch 3600/3665: Loss=2.5932
Epoch 3 | Train Loss: 2.5197 | Val Accuracy: 33.74%
Epoch 4, Batch 0/3665: Loss=2.6357
Epoch 4, Batch 150/3665: Loss=2.6401
Epoch 4, Batch 300/3665: Loss=2.9271
Epoch 4, Batch 450/3665: Loss=2.7302
Epoch 4, Batch 600/3665: Loss=2.7382
Epoch 4, Batch 750/3665: Loss=2.1805
Epoch 4, Batch 900/3665: Loss=2.0959
Epoch 4, Batch 1050/3665: Loss=2.4492
Epoch 4, Batch 1200/3665: Loss=2.3205
Epoch 4, Batch 1350/3665: Loss=2.2660
Epoch 4, Batch 1500/3665: Loss=2.0149
Epoch 4, Batch 1650/3665: Loss=2.4512
Epoch 4, Batch 1800/3665: Loss=2.1418
Epoch 4, Batch 1950/3665: Loss=2.9874
Epoch 4, Batch 2100/3665: Loss=2.5161
Epoch 4, Batch 2250/3665: Loss=2.2992
Epoch 4, Batch 2400/3665: Loss=2.1585
Epoch 4, Batch 2550/3665: Loss=2.3019
Epoch 4, Batch 2700/3665: Loss=2.0312
Epoch 4, Batch 2850/3665: Loss=2.1872
Epoch 4, Batch 3000/3665: Loss=2.2892
Epoch 4, Batch 3150/3665: Loss=2.2231
Epoch 4, Batch 3300/3665: Loss=2.0263
Epoch 4, Batch 3450/3665: Loss=2.6388
Epoch 4, Batch 3600/3665: Loss=2.0372
Epoch 4 | Train Loss: 2.3824 | Val Accuracy: 30.21%
Epoch 5, Batch 0/3665: Loss=1.9082
Epoch 5, Batch 150/3665: Loss=2.7707
Epoch 5, Batch 300/3665: Loss=1.8439
Epoch 5, Batch 450/3665: Loss=2.2305
Epoch 5, Batch 600/3665: Loss=2.2341
Epoch 5, Batch 750/3665: Loss=2.7768
Epoch 5, Batch 900/3665: Loss=2.5588
Epoch 5, Batch 1050/3665: Loss=2.6574
Epoch 5, Batch 1200/3665: Loss=2.4733
Epoch 5, Batch 1350/3665: Loss=2.8546
Epoch 5, Batch 1500/3665: Loss=2.2282
Epoch 5, Batch 1650/3665: Loss=2.0542
Epoch 5, Batch 1800/3665: Loss=2.4111
Epoch 5, Batch 1950/3665: Loss=1.9353
Epoch 5, Batch 2100/3665: Loss=2.0419
Epoch 5, Batch 2250/3665: Loss=2.1085
Epoch 5, Batch 2400/3665: Loss=2.0826
Epoch 5, Batch 2550/3665: Loss=3.0434
Epoch 5, Batch 2700/3665: Loss=2.2280
Epoch 5, Batch 2850/3665: Loss=2.0332
Epoch 5, Batch 3000/3665: Loss=2.6026
Epoch 5, Batch 3150/3665: Loss=1.8967
Epoch 5, Batch 3300/3665: Loss=2.7845
Epoch 5, Batch 3450/3665: Loss=2.2028
Epoch 5, Batch 3600/3665: Loss=2.1767
Epoch 5 | Train Loss: 2.3302 | Val Accuracy: 33.26%
Epoch 6, Batch 0/3665: Loss=1.7754
Epoch 6, Batch 150/3665: Loss=2.1654
Epoch 6, Batch 300/3665: Loss=2.5818
Epoch 6, Batch 450/3665: Loss=2.0225
Epoch 6, Batch 600/3665: Loss=2.5025
Epoch 6, Batch 750/3665: Loss=1.9137
Epoch 6, Batch 900/3665: Loss=2.4286
Epoch 6, Batch 1050/3665: Loss=2.1538
Epoch 6, Batch 1200/3665: Loss=2.3249
Epoch 6, Batch 1350/3665: Loss=2.1803
Epoch 6, Batch 1500/3665: Loss=2.0212
Epoch 6, Batch 1650/3665: Loss=2.5322
Epoch 6, Batch 1800/3665: Loss=2.1225
Epoch 6, Batch 1950/3665: Loss=2.0301
Epoch 6, Batch 2100/3665: Loss=2.2702
Epoch 6, Batch 2250/3665: Loss=2.3698
Epoch 6, Batch 2400/3665: Loss=2.1675
Epoch 6, Batch 2550/3665: Loss=2.5507
Epoch 6, Batch 2700/3665: Loss=2.5396
Epoch 6, Batch 2850/3665: Loss=2.2824
Epoch 6, Batch 3000/3665: Loss=2.2040
Epoch 6, Batch 3150/3665: Loss=2.3351
Epoch 6, Batch 3300/3665: Loss=2.1559
Epoch 6, Batch 3450/3665: Loss=2.2264
Epoch 6, Batch 3600/3665: Loss=2.2431
Epoch 6 | Train Loss: 2.2733 | Val Accuracy: 38.35%
Epoch 7, Batch 0/3665: Loss=1.9031
Epoch 7, Batch 150/3665: Loss=1.9443
Epoch 7, Batch 300/3665: Loss=2.1958
Epoch 7, Batch 450/3665: Loss=2.0793
Epoch 7, Batch 600/3665: Loss=2.8044
Epoch 7, Batch 750/3665: Loss=2.9359
Epoch 7, Batch 900/3665: Loss=2.3621
Epoch 7, Batch 1050/3665: Loss=1.7018
Epoch 7, Batch 1200/3665: Loss=2.4288
Epoch 7, Batch 1350/3665: Loss=2.4122
Epoch 7, Batch 1500/3665: Loss=1.9623
Epoch 7, Batch 1650/3665: Loss=2.4826
Epoch 7, Batch 1800/3665: Loss=1.6651
Epoch 7, Batch 1950/3665: Loss=1.8947
Epoch 7, Batch 2100/3665: Loss=1.5685
Epoch 7, Batch 2250/3665: Loss=2.6022
Epoch 7, Batch 2400/3665: Loss=2.3058
Epoch 7, Batch 2550/3665: Loss=2.1886
Epoch 7, Batch 2700/3665: Loss=2.4384
Epoch 7, Batch 2850/3665: Loss=2.3309
Epoch 7, Batch 3000/3665: Loss=1.9536
Epoch 7, Batch 3150/3665: Loss=1.8257
Epoch 7, Batch 3300/3665: Loss=2.5470
Epoch 7, Batch 3450/3665: Loss=2.3004
Epoch 7, Batch 3600/3665: Loss=2.4398
Epoch 7 | Train Loss: 2.1950 | Val Accuracy: 37.96%
Epoch 8, Batch 0/3665: Loss=2.4187
Epoch 8, Batch 150/3665: Loss=2.3383
Epoch 8, Batch 300/3665: Loss=1.8483
Epoch 8, Batch 450/3665: Loss=2.2969
Epoch 8, Batch 600/3665: Loss=1.4692
Epoch 8, Batch 750/3665: Loss=2.2524
Epoch 8, Batch 900/3665: Loss=2.1923
Epoch 8, Batch 1050/3665: Loss=2.7155
Epoch 8, Batch 1200/3665: Loss=2.3554
Epoch 8, Batch 1350/3665: Loss=2.2530
Epoch 8, Batch 1500/3665: Loss=1.8207
Epoch 8, Batch 1650/3665: Loss=1.4977
Epoch 8, Batch 1800/3665: Loss=2.1949
Epoch 8, Batch 1950/3665: Loss=2.0449
Epoch 8, Batch 2100/3665: Loss=1.8516
Epoch 8, Batch 2250/3665: Loss=2.6715
Epoch 8, Batch 2400/3665: Loss=1.8817
Epoch 8, Batch 2550/3665: Loss=2.2901
Epoch 8, Batch 2700/3665: Loss=2.2524
Epoch 8, Batch 2850/3665: Loss=2.1867
Epoch 8, Batch 3000/3665: Loss=2.1583
Epoch 8, Batch 3150/3665: Loss=1.8243
Epoch 8, Batch 3300/3665: Loss=2.0434
Epoch 8, Batch 3450/3665: Loss=2.1180
Epoch 8, Batch 3600/3665: Loss=1.8224
Epoch 8 | Train Loss: 2.0634 | Val Accuracy: 41.07%
Epoch 9, Batch 0/3665: Loss=2.0332
Epoch 9, Batch 150/3665: Loss=1.8195
Epoch 9, Batch 300/3665: Loss=2.1914
Epoch 9, Batch 450/3665: Loss=2.2731
Epoch 9, Batch 600/3665: Loss=1.8050
Epoch 9, Batch 750/3665: Loss=1.9767
Epoch 9, Batch 900/3665: Loss=1.8315
Epoch 9, Batch 1050/3665: Loss=1.6295
Epoch 9, Batch 1200/3665: Loss=1.3587
Epoch 9, Batch 1350/3665: Loss=2.1908
Epoch 9, Batch 1500/3665: Loss=1.6733
Epoch 9, Batch 1650/3665: Loss=1.5753
Epoch 9, Batch 1800/3665: Loss=2.2068
Epoch 9, Batch 1950/3665: Loss=1.9291
Epoch 9, Batch 2100/3665: Loss=2.2253
Epoch 9, Batch 2250/3665: Loss=1.6721
Epoch 9, Batch 2400/3665: Loss=1.9906
Epoch 9, Batch 2550/3665: Loss=2.4807
Epoch 9, Batch 2700/3665: Loss=1.9792
Epoch 9, Batch 2850/3665: Loss=1.7679
Epoch 9, Batch 3000/3665: Loss=1.8982
Epoch 9, Batch 3150/3665: Loss=1.4484
Epoch 9, Batch 3300/3665: Loss=2.3347
Epoch 9, Batch 3450/3665: Loss=2.0037
Epoch 9, Batch 3600/3665: Loss=1.5264
Epoch 9 | Train Loss: 1.9519 | Val Accuracy: 42.33%
Final Val Accuracy: 42.33%

--- QI ResNet101 without Jet Loss 1F ---
Epoch 0, Batch 0/3665: Loss=4.3989
Epoch 0, Batch 50/3665: Loss=3.3877
Epoch 0, Batch 100/3665: Loss=3.4219
Epoch 0, Batch 150/3665: Loss=3.6328
Epoch 0, Batch 200/3665: Loss=3.5729
Epoch 0, Batch 250/3665: Loss=3.2330
Epoch 0, Batch 300/3665: Loss=4.0283
Epoch 0, Batch 350/3665: Loss=3.4010
Epoch 0, Batch 400/3665: Loss=3.2497
Epoch 0, Batch 450/3665: Loss=3.4974
Epoch 0, Batch 500/3665: Loss=3.3262
Epoch 0, Batch 550/3665: Loss=3.4197
Epoch 0, Batch 600/3665: Loss=3.2728
Epoch 0, Batch 650/3665: Loss=3.9735
Epoch 0, Batch 700/3665: Loss=3.6692
Epoch 0, Batch 750/3665: Loss=3.1378
Epoch 0, Batch 800/3665: Loss=3.4797
Epoch 0, Batch 850/3665: Loss=3.4839
Epoch 0, Batch 900/3665: Loss=3.6836
Epoch 0, Batch 950/3665: Loss=3.5246
Epoch 0, Batch 1000/3665: Loss=3.1973
Epoch 0, Batch 1050/3665: Loss=3.2573
Epoch 0, Batch 1100/3665: Loss=3.6484
Epoch 0, Batch 1150/3665: Loss=3.3783
Epoch 0, Batch 1200/3665: Loss=3.1724
Epoch 0, Batch 1250/3665: Loss=3.5140
Epoch 0, Batch 1300/3665: Loss=3.1919
Epoch 0, Batch 1350/3665: Loss=3.4096
Epoch 0, Batch 1400/3665: Loss=3.1982
Epoch 0, Batch 1450/3665: Loss=3.5050
Epoch 0, Batch 1500/3665: Loss=3.6610
Epoch 0, Batch 1550/3665: Loss=3.8046
Epoch 0, Batch 1600/3665: Loss=3.5150
Epoch 0, Batch 1650/3665: Loss=3.2682
Epoch 0, Batch 1700/3665: Loss=3.1555
Epoch 0, Batch 1750/3665: Loss=3.0300
Epoch 0, Batch 1800/3665: Loss=3.2308
Epoch 0, Batch 1850/3665: Loss=3.0800
Epoch 0, Batch 1900/3665: Loss=3.7609
Epoch 0, Batch 1950/3665: Loss=3.3476
Epoch 0, Batch 2000/3665: Loss=3.2462
Epoch 0, Batch 2050/3665: Loss=3.2007
Epoch 0, Batch 2100/3665: Loss=3.0320
Epoch 0, Batch 2150/3665: Loss=3.2918
Epoch 0, Batch 2200/3665: Loss=3.0424
Epoch 0, Batch 2250/3665: Loss=3.0425
Epoch 0, Batch 2300/3665: Loss=3.7148
Epoch 0, Batch 2350/3665: Loss=3.3898
Epoch 0, Batch 2400/3665: Loss=3.1049
Epoch 0, Batch 2450/3665: Loss=3.3556
Epoch 0, Batch 2500/3665: Loss=2.9707
Epoch 0, Batch 2550/3665: Loss=2.8995
Epoch 0, Batch 2600/3665: Loss=3.2013
Epoch 0, Batch 2650/3665: Loss=2.9057
Epoch 0, Batch 2700/3665: Loss=2.7171
Epoch 0, Batch 2750/3665: Loss=2.9704
Epoch 0, Batch 2800/3665: Loss=3.3540
Epoch 0, Batch 2850/3665: Loss=3.3166
Epoch 0, Batch 2900/3665: Loss=2.7301
Epoch 0, Batch 2950/3665: Loss=3.5693
Epoch 0, Batch 3000/3665: Loss=3.2278
Epoch 0, Batch 3050/3665: Loss=3.7963
Epoch 0, Batch 3100/3665: Loss=2.7436
Epoch 0, Batch 3150/3665: Loss=2.6302
Epoch 0, Batch 3200/3665: Loss=3.3262
Epoch 0, Batch 3250/3665: Loss=3.0446
Epoch 0, Batch 3300/3665: Loss=2.7109
Epoch 0, Batch 3350/3665: Loss=3.6760
Epoch 0, Batch 3400/3665: Loss=3.2175
Epoch 0, Batch 3450/3665: Loss=2.8511
Epoch 0, Batch 3500/3665: Loss=2.9169
Epoch 0, Batch 3550/3665: Loss=3.2774
Epoch 0, Batch 3600/3665: Loss=3.4754
Epoch 0, Batch 3650/3665: Loss=3.2329
Epoch 0 | Train Loss: 3.3425 | Val Accuracy: 22.33%
Epoch 1, Batch 0/3665: Loss=3.1132
Epoch 1, Batch 50/3665: Loss=3.2806
Epoch 1, Batch 100/3665: Loss=2.8107
Epoch 1, Batch 150/3665: Loss=3.2881
Epoch 1, Batch 200/3665: Loss=3.1232
Epoch 1, Batch 250/3665: Loss=3.1633
Epoch 1, Batch 300/3665: Loss=2.8717
Epoch 1, Batch 350/3665: Loss=3.3769
Epoch 1, Batch 400/3665: Loss=2.7739
Epoch 1, Batch 450/3665: Loss=3.1088
Epoch 1, Batch 500/3665: Loss=3.1216
Epoch 1, Batch 550/3665: Loss=3.5634
Epoch 1, Batch 600/3665: Loss=3.2226
Epoch 1, Batch 650/3665: Loss=3.0954
Epoch 1, Batch 700/3665: Loss=2.4290
Epoch 1, Batch 750/3665: Loss=2.8374
Epoch 1, Batch 800/3665: Loss=2.9609
Epoch 1, Batch 850/3665: Loss=2.9236
Epoch 1, Batch 900/3665: Loss=2.8674
Epoch 1, Batch 950/3665: Loss=3.1787
Epoch 1, Batch 1000/3665: Loss=3.1485
Epoch 1, Batch 1050/3665: Loss=3.3467
Epoch 1, Batch 1100/3665: Loss=3.0495
Epoch 1, Batch 1150/3665: Loss=3.0287
Epoch 1, Batch 1200/3665: Loss=2.9344
Epoch 1, Batch 1250/3665: Loss=2.4953
Epoch 1, Batch 1300/3665: Loss=3.1372
Epoch 1, Batch 1350/3665: Loss=2.9841
Epoch 1, Batch 1400/3665: Loss=3.4081
Epoch 1, Batch 1450/3665: Loss=2.8024
Epoch 1, Batch 1500/3665: Loss=2.8272
Epoch 1, Batch 1550/3665: Loss=2.6042
Epoch 1, Batch 1600/3665: Loss=3.3460
Epoch 1, Batch 1650/3665: Loss=2.7704
Epoch 1, Batch 1700/3665: Loss=3.1124
Epoch 1, Batch 1750/3665: Loss=3.0850
Epoch 1, Batch 1800/3665: Loss=2.8177
Epoch 1, Batch 1850/3665: Loss=2.7089
Epoch 1, Batch 1900/3665: Loss=2.8369
Epoch 1, Batch 1950/3665: Loss=2.6508
Epoch 1, Batch 2000/3665: Loss=2.6495
Epoch 1, Batch 2050/3665: Loss=2.1592
Epoch 1, Batch 2100/3665: Loss=2.4204
Epoch 1, Batch 2150/3665: Loss=3.4703
Epoch 1, Batch 2200/3665: Loss=3.4039
Epoch 1, Batch 2250/3665: Loss=2.6662
Epoch 1, Batch 2300/3665: Loss=3.0447
Epoch 1, Batch 2350/3665: Loss=3.2171
Epoch 1, Batch 2400/3665: Loss=3.0944
Epoch 1, Batch 2450/3665: Loss=3.0007
Epoch 1, Batch 2500/3665: Loss=3.1681
Epoch 1, Batch 2550/3665: Loss=2.6349
Epoch 1, Batch 2600/3665: Loss=2.6684
Epoch 1, Batch 2650/3665: Loss=2.7730
Epoch 1, Batch 2700/3665: Loss=2.7712
Epoch 1, Batch 2750/3665: Loss=2.8227
Epoch 1, Batch 2800/3665: Loss=2.0416
Epoch 1, Batch 2850/3665: Loss=2.7568
Epoch 1, Batch 2900/3665: Loss=2.5079
Epoch 1, Batch 2950/3665: Loss=3.3826
Epoch 1, Batch 3000/3665: Loss=2.6344
Epoch 1, Batch 3050/3665: Loss=2.7690
Epoch 1, Batch 3100/3665: Loss=2.7749
Epoch 1, Batch 3150/3665: Loss=2.7495
Epoch 1, Batch 3200/3665: Loss=2.6957
Epoch 1, Batch 3250/3665: Loss=2.6229
Epoch 1, Batch 3300/3665: Loss=2.4452
Epoch 1, Batch 3350/3665: Loss=2.8711
Epoch 1, Batch 3400/3665: Loss=2.9648
Epoch 1, Batch 3450/3665: Loss=2.6351
Epoch 1, Batch 3500/3665: Loss=2.7169
Epoch 1, Batch 3550/3665: Loss=2.6244
Epoch 1, Batch 3600/3665: Loss=2.4728
Epoch 1, Batch 3650/3665: Loss=2.5735
Epoch 1 | Train Loss: 2.9094 | Val Accuracy: 28.09%
Epoch 2, Batch 0/3665: Loss=2.3665
Epoch 2, Batch 50/3665: Loss=2.7843
Epoch 2, Batch 100/3665: Loss=3.2384
Epoch 2, Batch 150/3665: Loss=2.5258
Epoch 2, Batch 200/3665: Loss=2.6913
Epoch 2, Batch 250/3665: Loss=2.6342
Epoch 2, Batch 300/3665: Loss=2.6878
Epoch 2, Batch 350/3665: Loss=2.4215
Epoch 2, Batch 400/3665: Loss=2.8767
Epoch 2, Batch 450/3665: Loss=2.5210
Epoch 2, Batch 500/3665: Loss=2.8128
Epoch 2, Batch 550/3665: Loss=2.9258
Epoch 2, Batch 600/3665: Loss=3.2922
Epoch 2, Batch 650/3665: Loss=2.4202
Epoch 2, Batch 700/3665: Loss=2.8039
Epoch 2, Batch 750/3665: Loss=2.4588
Epoch 2, Batch 800/3665: Loss=2.8558
Epoch 2, Batch 850/3665: Loss=3.1683
Epoch 2, Batch 900/3665: Loss=3.5368
Epoch 2, Batch 950/3665: Loss=2.8058
Epoch 2, Batch 1000/3665: Loss=2.8985
Epoch 2, Batch 1050/3665: Loss=3.0722
Epoch 2, Batch 1100/3665: Loss=2.5844
Epoch 2, Batch 1150/3665: Loss=2.6688
Epoch 2, Batch 1200/3665: Loss=2.8351
Epoch 2, Batch 1250/3665: Loss=2.4580
Epoch 2, Batch 1300/3665: Loss=2.3752
Epoch 2, Batch 1350/3665: Loss=2.9878
Epoch 2, Batch 1400/3665: Loss=2.7859
Epoch 2, Batch 1450/3665: Loss=2.4660
Epoch 2, Batch 1500/3665: Loss=2.9634
Epoch 2, Batch 1550/3665: Loss=2.5988
Epoch 2, Batch 1600/3665: Loss=2.4576
Epoch 2, Batch 1650/3665: Loss=2.2752
Epoch 2, Batch 1700/3665: Loss=2.6184
Epoch 2, Batch 1750/3665: Loss=2.3262
Epoch 2, Batch 1800/3665: Loss=2.7082
Epoch 2, Batch 1850/3665: Loss=2.8174
Epoch 2, Batch 1900/3665: Loss=2.7611
Epoch 2, Batch 1950/3665: Loss=2.5923
Epoch 2, Batch 2000/3665: Loss=2.8999
Epoch 2, Batch 2050/3665: Loss=2.7405
Epoch 2, Batch 2100/3665: Loss=2.7566
Epoch 2, Batch 2150/3665: Loss=2.7222
Epoch 2, Batch 2200/3665: Loss=3.1731
Epoch 2, Batch 2250/3665: Loss=2.3913
Epoch 2, Batch 2300/3665: Loss=2.8720
Epoch 2, Batch 2350/3665: Loss=2.4605
Epoch 2, Batch 2400/3665: Loss=2.6067
Epoch 2, Batch 2450/3665: Loss=2.5728
Epoch 2, Batch 2500/3665: Loss=2.6167
Epoch 2, Batch 2550/3665: Loss=2.6519
Epoch 2, Batch 2600/3665: Loss=2.8678
Epoch 2, Batch 2650/3665: Loss=2.7023
Epoch 2, Batch 2700/3665: Loss=2.6267
Epoch 2, Batch 2750/3665: Loss=2.4911
Epoch 2, Batch 2800/3665: Loss=2.2198
Epoch 2, Batch 2850/3665: Loss=3.0011
Epoch 2, Batch 2900/3665: Loss=2.0365
Epoch 2, Batch 2950/3665: Loss=1.8980
Epoch 2, Batch 3000/3665: Loss=2.6948
Epoch 2, Batch 3050/3665: Loss=2.8157
Epoch 2, Batch 3100/3665: Loss=3.0218
Epoch 2, Batch 3150/3665: Loss=2.5106
Epoch 2, Batch 3200/3665: Loss=2.8903
Epoch 2, Batch 3250/3665: Loss=2.8463
Epoch 2, Batch 3300/3665: Loss=2.7513
Epoch 2, Batch 3350/3665: Loss=2.3444
Epoch 2, Batch 3400/3665: Loss=2.5536
Epoch 2, Batch 3450/3665: Loss=2.6018
Epoch 2, Batch 3500/3665: Loss=2.4072
Epoch 2, Batch 3550/3665: Loss=2.3744
Epoch 2, Batch 3600/3665: Loss=2.8453
Epoch 2, Batch 3650/3665: Loss=2.5090
Epoch 2 | Train Loss: 2.6602 | Val Accuracy: 32.82%
Epoch 3, Batch 0/3665: Loss=2.6199
Epoch 3, Batch 50/3665: Loss=2.2547
Epoch 3, Batch 100/3665: Loss=2.9650
Epoch 3, Batch 150/3665: Loss=2.4875
Epoch 3, Batch 200/3665: Loss=2.6543
Epoch 3, Batch 250/3665: Loss=2.4412
Epoch 3, Batch 300/3665: Loss=2.3356
Epoch 3, Batch 350/3665: Loss=2.3292
Epoch 3, Batch 400/3665: Loss=2.2734
Epoch 3, Batch 450/3665: Loss=2.0718
Epoch 3, Batch 500/3665: Loss=2.5477
Epoch 3, Batch 550/3665: Loss=2.2641
Epoch 3, Batch 600/3665: Loss=2.4078
Epoch 3, Batch 650/3665: Loss=2.7108
Epoch 3, Batch 700/3665: Loss=1.9706
Epoch 3, Batch 750/3665: Loss=2.6577
Epoch 3, Batch 800/3665: Loss=2.5642
Epoch 3, Batch 850/3665: Loss=2.3698
Epoch 3, Batch 900/3665: Loss=2.3589
Epoch 3, Batch 950/3665: Loss=2.5860
Epoch 3, Batch 1000/3665: Loss=2.6549
Epoch 3, Batch 1050/3665: Loss=2.3985
Epoch 3, Batch 1100/3665: Loss=2.3421
Epoch 3, Batch 1150/3665: Loss=2.1897
Epoch 3, Batch 1200/3665: Loss=2.0468
Epoch 3, Batch 1250/3665: Loss=2.0916
Epoch 3, Batch 1300/3665: Loss=2.7015
Epoch 3, Batch 1350/3665: Loss=2.6637
Epoch 3, Batch 1400/3665: Loss=2.5922
Epoch 3, Batch 1450/3665: Loss=2.4023
Epoch 3, Batch 1500/3665: Loss=2.3230
Epoch 3, Batch 1550/3665: Loss=2.6558
Epoch 3, Batch 1600/3665: Loss=2.8674
Epoch 3, Batch 1650/3665: Loss=2.6195
Epoch 3, Batch 1700/3665: Loss=2.3638
Epoch 3, Batch 1750/3665: Loss=2.3936
Epoch 3, Batch 1800/3665: Loss=2.1687
Epoch 3, Batch 1850/3665: Loss=2.7132
Epoch 3, Batch 1900/3665: Loss=2.5885
Epoch 3, Batch 1950/3665: Loss=1.9224
Epoch 3, Batch 2000/3665: Loss=2.4165
Epoch 3, Batch 2050/3665: Loss=2.7189
Epoch 3, Batch 2100/3665: Loss=2.0839
Epoch 3, Batch 2150/3665: Loss=2.7377
Epoch 3, Batch 2200/3665: Loss=2.5327
Epoch 3, Batch 2250/3665: Loss=2.1330
Epoch 3, Batch 2300/3665: Loss=2.4094
Epoch 3, Batch 2350/3665: Loss=2.4057
Epoch 3, Batch 2400/3665: Loss=2.3028
Epoch 3, Batch 2450/3665: Loss=2.2870
Epoch 3, Batch 2500/3665: Loss=2.4535
Epoch 3, Batch 2550/3665: Loss=2.1140
Epoch 3, Batch 2600/3665: Loss=2.5294
Epoch 3, Batch 2650/3665: Loss=2.7838
Epoch 3, Batch 2700/3665: Loss=2.5324
Epoch 3, Batch 2750/3665: Loss=2.3914
Epoch 3, Batch 2800/3665: Loss=2.4328
Epoch 3, Batch 2850/3665: Loss=2.8444
Epoch 3, Batch 2900/3665: Loss=2.1784
Epoch 3, Batch 2950/3665: Loss=2.6252
Epoch 3, Batch 3000/3665: Loss=2.9076
Epoch 3, Batch 3050/3665: Loss=2.5853
Epoch 3, Batch 3100/3665: Loss=2.7018
Epoch 3, Batch 3150/3665: Loss=2.6693
Epoch 3, Batch 3200/3665: Loss=2.2785
Epoch 3, Batch 3250/3665: Loss=2.1121
Epoch 3, Batch 3300/3665: Loss=2.4683
Epoch 3, Batch 3350/3665: Loss=1.9111
Epoch 3, Batch 3400/3665: Loss=2.3828
Epoch 3, Batch 3450/3665: Loss=2.2379
Epoch 3, Batch 3500/3665: Loss=3.0802
Epoch 3, Batch 3550/3665: Loss=2.2482
Epoch 3, Batch 3600/3665: Loss=2.4424
Epoch 3, Batch 3650/3665: Loss=2.6797
Epoch 3 | Train Loss: 2.4802 | Val Accuracy: 36.39%
Epoch 4, Batch 0/3665: Loss=2.0492
Epoch 4, Batch 50/3665: Loss=2.4003
Epoch 4, Batch 100/3665: Loss=2.4593
Epoch 4, Batch 150/3665: Loss=2.4945
Epoch 4, Batch 200/3665: Loss=2.5143
Epoch 4, Batch 250/3665: Loss=2.5404
Epoch 4, Batch 300/3665: Loss=2.2447
Epoch 4, Batch 350/3665: Loss=2.5405
Epoch 4, Batch 400/3665: Loss=2.2648
Epoch 4, Batch 450/3665: Loss=2.3129
Epoch 4, Batch 500/3665: Loss=3.0478
Epoch 4, Batch 550/3665: Loss=2.7886
Epoch 4, Batch 600/3665: Loss=2.3448
Epoch 4, Batch 650/3665: Loss=2.6114
Epoch 4, Batch 700/3665: Loss=2.2684
Epoch 4, Batch 750/3665: Loss=2.4280
Epoch 4, Batch 800/3665: Loss=2.8599
Epoch 4, Batch 850/3665: Loss=2.3976
Epoch 4, Batch 900/3665: Loss=2.1440
Epoch 4, Batch 950/3665: Loss=2.1920
Epoch 4, Batch 1000/3665: Loss=2.5861
Epoch 4, Batch 1050/3665: Loss=2.4270
Epoch 4, Batch 1100/3665: Loss=2.4082
Epoch 4, Batch 1150/3665: Loss=2.2606
Epoch 4, Batch 1200/3665: Loss=2.6809
Epoch 4, Batch 1250/3665: Loss=2.1566
Epoch 4, Batch 1300/3665: Loss=2.1527
Epoch 4, Batch 1350/3665: Loss=2.1200
Epoch 4, Batch 1400/3665: Loss=2.3654
Epoch 4, Batch 1450/3665: Loss=2.5679
Epoch 4, Batch 1500/3665: Loss=2.5961
Epoch 4, Batch 1550/3665: Loss=2.4071
Epoch 4, Batch 1600/3665: Loss=2.8683
Epoch 4, Batch 1650/3665: Loss=3.2454
Epoch 4, Batch 1700/3665: Loss=2.2542
Epoch 4, Batch 1750/3665: Loss=2.4523
Epoch 4, Batch 1800/3665: Loss=2.3620
Epoch 4, Batch 1850/3665: Loss=2.7380
Epoch 4, Batch 1900/3665: Loss=2.1768
Epoch 4, Batch 1950/3665: Loss=2.4186
Epoch 4, Batch 2000/3665: Loss=1.7999
Epoch 4, Batch 2050/3665: Loss=2.4618
Epoch 4, Batch 2100/3665: Loss=2.7962
Epoch 4, Batch 2150/3665: Loss=2.7619
Epoch 4, Batch 2200/3665: Loss=2.0135
Epoch 4, Batch 2250/3665: Loss=2.3139
Epoch 4, Batch 2300/3665: Loss=2.3173
Epoch 4, Batch 2350/3665: Loss=2.5715
Epoch 4, Batch 2400/3665: Loss=2.7185
Epoch 4, Batch 2450/3665: Loss=2.5357
Epoch 4, Batch 2500/3665: Loss=2.8552
Epoch 4, Batch 2550/3665: Loss=2.3253
Epoch 4, Batch 2600/3665: Loss=1.9832
Epoch 4, Batch 2650/3665: Loss=2.7554
Epoch 4, Batch 2700/3665: Loss=1.9167
Epoch 4, Batch 2750/3665: Loss=2.0180
Epoch 4, Batch 2800/3665: Loss=2.4948
Epoch 4, Batch 2850/3665: Loss=2.0151
Epoch 4, Batch 2900/3665: Loss=2.1010
Epoch 4, Batch 2950/3665: Loss=1.7047
Epoch 4, Batch 3000/3665: Loss=1.8414
Epoch 4, Batch 3050/3665: Loss=2.3636
Epoch 4, Batch 3100/3665: Loss=2.6537
Epoch 4, Batch 3150/3665: Loss=2.1998
Epoch 4, Batch 3200/3665: Loss=1.8673
Epoch 4, Batch 3250/3665: Loss=2.1875
Epoch 4, Batch 3300/3665: Loss=2.2897
Epoch 4, Batch 3350/3665: Loss=1.8131
Epoch 4, Batch 3400/3665: Loss=2.1638
Epoch 4, Batch 3450/3665: Loss=2.7293
Epoch 4, Batch 3500/3665: Loss=2.3856
Epoch 4, Batch 3550/3665: Loss=1.7460
Epoch 4, Batch 3600/3665: Loss=2.2137
Epoch 4, Batch 3650/3665: Loss=2.2403
Epoch 4 | Train Loss: 2.3469 | Val Accuracy: 37.90%
Epoch 5, Batch 0/3665: Loss=1.6525
Epoch 5, Batch 50/3665: Loss=2.1692
Epoch 5, Batch 100/3665: Loss=2.4147
Epoch 5, Batch 150/3665: Loss=2.1626
Epoch 5, Batch 200/3665: Loss=2.0809
Epoch 5, Batch 250/3665: Loss=2.3122
Epoch 5, Batch 300/3665: Loss=2.2792
Epoch 5, Batch 350/3665: Loss=1.8889
Epoch 5, Batch 400/3665: Loss=1.9196
Epoch 5, Batch 450/3665: Loss=1.7354
Epoch 5, Batch 500/3665: Loss=2.4637
Epoch 5, Batch 550/3665: Loss=1.9047
Epoch 5, Batch 600/3665: Loss=2.8405
Epoch 5, Batch 650/3665: Loss=2.7135
Epoch 5, Batch 700/3665: Loss=2.0353
Epoch 5, Batch 750/3665: Loss=2.2046
Epoch 5, Batch 800/3665: Loss=2.2731
Epoch 5, Batch 850/3665: Loss=2.0901
Epoch 5, Batch 900/3665: Loss=2.2659
Epoch 5, Batch 950/3665: Loss=2.3099
Epoch 5, Batch 1000/3665: Loss=2.3011
Epoch 5, Batch 1050/3665: Loss=2.1602
Epoch 5, Batch 1100/3665: Loss=2.6566
Epoch 5, Batch 1150/3665: Loss=2.4429
Epoch 5, Batch 1200/3665: Loss=2.2009
Epoch 5, Batch 1250/3665: Loss=2.2735
Epoch 5, Batch 1300/3665: Loss=1.6715
Epoch 5, Batch 1350/3665: Loss=1.9005
Epoch 5, Batch 1400/3665: Loss=2.2455
Epoch 5, Batch 1450/3665: Loss=2.3526
Epoch 5, Batch 1500/3665: Loss=2.7201
Epoch 5, Batch 1550/3665: Loss=1.9152
Epoch 5, Batch 1600/3665: Loss=2.2115
Epoch 5, Batch 1650/3665: Loss=1.9983
Epoch 5, Batch 1700/3665: Loss=2.1860
Epoch 5, Batch 1750/3665: Loss=2.0127
Epoch 5, Batch 1800/3665: Loss=1.9167
Epoch 5, Batch 1850/3665: Loss=2.2997
Epoch 5, Batch 1900/3665: Loss=1.8869
Epoch 5, Batch 1950/3665: Loss=2.0529
Epoch 5, Batch 2000/3665: Loss=2.7248
Epoch 5, Batch 2050/3665: Loss=2.2151
Epoch 5, Batch 2100/3665: Loss=2.6623
Epoch 5, Batch 2150/3665: Loss=2.3349
Epoch 5, Batch 2200/3665: Loss=2.2334
Epoch 5, Batch 2250/3665: Loss=2.4890
Epoch 5, Batch 2300/3665: Loss=2.2740
Epoch 5, Batch 2350/3665: Loss=2.2956
Epoch 5, Batch 2400/3665: Loss=3.2381
Epoch 5, Batch 2450/3665: Loss=2.2477
Epoch 5, Batch 2500/3665: Loss=1.6628
Epoch 5, Batch 2550/3665: Loss=1.8949
Epoch 5, Batch 2600/3665: Loss=2.4101
Epoch 5, Batch 2650/3665: Loss=2.7681
Epoch 5, Batch 2700/3665: Loss=2.3030
Epoch 5, Batch 2750/3665: Loss=2.4821
Epoch 5, Batch 2800/3665: Loss=1.9944
Epoch 5, Batch 2850/3665: Loss=2.0393
Epoch 5, Batch 2900/3665: Loss=2.1610
Epoch 5, Batch 2950/3665: Loss=2.3886
Epoch 5, Batch 3000/3665: Loss=1.8560
Epoch 5, Batch 3050/3665: Loss=2.5762
Epoch 5, Batch 3100/3665: Loss=2.3644
Epoch 5, Batch 3150/3665: Loss=1.9018
Epoch 5, Batch 3200/3665: Loss=2.1104
Epoch 5, Batch 3250/3665: Loss=2.3151
Epoch 5, Batch 3300/3665: Loss=2.1540
Epoch 5, Batch 3350/3665: Loss=2.1328
Epoch 5, Batch 3400/3665: Loss=2.2584
Epoch 5, Batch 3450/3665: Loss=1.4614
Epoch 5, Batch 3500/3665: Loss=2.4741
Epoch 5, Batch 3550/3665: Loss=2.7295
Epoch 5, Batch 3600/3665: Loss=2.0464
Epoch 5, Batch 3650/3665: Loss=2.5193
Epoch 5 | Train Loss: 2.2535 | Val Accuracy: 38.39%
Epoch 6, Batch 0/3665: Loss=2.5785
Epoch 6, Batch 50/3665: Loss=2.3203
Epoch 6, Batch 100/3665: Loss=2.0337
Epoch 6, Batch 150/3665: Loss=2.4960
Epoch 6, Batch 200/3665: Loss=1.8286
Epoch 6, Batch 250/3665: Loss=2.4288
Epoch 6, Batch 300/3665: Loss=2.1114
Epoch 6, Batch 350/3665: Loss=2.0638
Epoch 6, Batch 400/3665: Loss=2.5124
Epoch 6, Batch 450/3665: Loss=2.0007
Epoch 6, Batch 500/3665: Loss=2.7186
Epoch 6, Batch 550/3665: Loss=2.2785
Epoch 6, Batch 600/3665: Loss=2.0266
Epoch 6, Batch 650/3665: Loss=2.5590
Epoch 6, Batch 700/3665: Loss=2.1048
Epoch 6, Batch 750/3665: Loss=1.9209
Epoch 6, Batch 800/3665: Loss=2.7890
Epoch 6, Batch 850/3665: Loss=1.9923
Epoch 6, Batch 900/3665: Loss=1.7364
Epoch 6, Batch 950/3665: Loss=1.9181
Epoch 6, Batch 1000/3665: Loss=1.5897
Epoch 6, Batch 1050/3665: Loss=2.4065
Epoch 6, Batch 1100/3665: Loss=2.4785
Epoch 6, Batch 1150/3665: Loss=2.3682
Epoch 6, Batch 1200/3665: Loss=1.9767
Epoch 6, Batch 1250/3665: Loss=1.6866
Epoch 6, Batch 1300/3665: Loss=2.0092
Epoch 6, Batch 1350/3665: Loss=2.2180
Epoch 6, Batch 1400/3665: Loss=2.4359
Epoch 6, Batch 1450/3665: Loss=1.7911
Epoch 6, Batch 1500/3665: Loss=2.5372
Epoch 6, Batch 1550/3665: Loss=1.5444
Epoch 6, Batch 1600/3665: Loss=2.2515
Epoch 6, Batch 1650/3665: Loss=2.0621
Epoch 6, Batch 1700/3665: Loss=1.9345
Epoch 6, Batch 1750/3665: Loss=2.7924
Epoch 6, Batch 1800/3665: Loss=2.2568
Epoch 6, Batch 1850/3665: Loss=2.3967
Epoch 6, Batch 1900/3665: Loss=2.3352
Epoch 6, Batch 1950/3665: Loss=2.1230
Epoch 6, Batch 2000/3665: Loss=2.1401
Epoch 6, Batch 2050/3665: Loss=2.2789
Epoch 6, Batch 2100/3665: Loss=3.2356
Epoch 6, Batch 2150/3665: Loss=2.2093
Epoch 6, Batch 2200/3665: Loss=2.2640
Epoch 6, Batch 2250/3665: Loss=1.8581
Epoch 6, Batch 2300/3665: Loss=1.9626
Epoch 6, Batch 2350/3665: Loss=2.4250
Epoch 6, Batch 2400/3665: Loss=2.0620
Epoch 6, Batch 2450/3665: Loss=1.9858
Epoch 6, Batch 2500/3665: Loss=1.7738
Epoch 6, Batch 2550/3665: Loss=1.9581
Epoch 6, Batch 2600/3665: Loss=1.6478
Epoch 6, Batch 2650/3665: Loss=2.2417
Epoch 6, Batch 2700/3665: Loss=1.8716
Epoch 6, Batch 2750/3665: Loss=2.1265
Epoch 6, Batch 2800/3665: Loss=2.0302
Epoch 6, Batch 2850/3665: Loss=2.1566
Epoch 6, Batch 2900/3665: Loss=2.3227
Epoch 6, Batch 2950/3665: Loss=1.7459
Epoch 6, Batch 3000/3665: Loss=1.4789
Epoch 6, Batch 3050/3665: Loss=1.8264
Epoch 6, Batch 3100/3665: Loss=2.3469
Epoch 6, Batch 3150/3665: Loss=2.2449
Epoch 6, Batch 3200/3665: Loss=2.0087
Epoch 6, Batch 3250/3665: Loss=2.0612
Epoch 6, Batch 3300/3665: Loss=2.0184
Epoch 6, Batch 3350/3665: Loss=1.7454
Epoch 6, Batch 3400/3665: Loss=1.6097
Epoch 6, Batch 3450/3665: Loss=1.9089
Epoch 6, Batch 3500/3665: Loss=1.9800
Epoch 6, Batch 3550/3665: Loss=2.0027
Epoch 6, Batch 3600/3665: Loss=1.8957
Epoch 6, Batch 3650/3665: Loss=2.4468
Epoch 6 | Train Loss: 2.1208 | Val Accuracy: 41.86%
Epoch 7, Batch 0/3665: Loss=1.8828
Epoch 7, Batch 50/3665: Loss=2.6546
Epoch 7, Batch 100/3665: Loss=1.6818
Epoch 7, Batch 150/3665: Loss=2.0226
Epoch 7, Batch 200/3665: Loss=2.1184
Epoch 7, Batch 250/3665: Loss=1.4563
Epoch 7, Batch 300/3665: Loss=2.4257
Epoch 7, Batch 350/3665: Loss=2.1295
Epoch 7, Batch 400/3665: Loss=1.5327
Epoch 7, Batch 450/3665: Loss=2.7210
Epoch 7, Batch 500/3665: Loss=1.8882
Epoch 7, Batch 550/3665: Loss=1.7787
Epoch 7, Batch 600/3665: Loss=1.5968
Epoch 7, Batch 650/3665: Loss=2.1963
Epoch 7, Batch 700/3665: Loss=1.6552
Epoch 7, Batch 750/3665: Loss=1.9437
Epoch 7, Batch 800/3665: Loss=2.2011
Epoch 7, Batch 850/3665: Loss=2.5290
Epoch 7, Batch 900/3665: Loss=1.6452
Epoch 7, Batch 950/3665: Loss=2.3974
Epoch 7, Batch 1000/3665: Loss=2.2000
Epoch 7, Batch 1050/3665: Loss=2.3654
Epoch 7, Batch 1100/3665: Loss=1.5694
Epoch 7, Batch 1150/3665: Loss=1.7654
Epoch 7, Batch 1200/3665: Loss=2.1270
Epoch 7, Batch 1250/3665: Loss=1.9502
Epoch 7, Batch 1300/3665: Loss=2.0302
Epoch 7, Batch 1350/3665: Loss=2.3450
Epoch 7, Batch 1400/3665: Loss=1.7806
Epoch 7, Batch 1450/3665: Loss=1.7333
Epoch 7, Batch 1500/3665: Loss=2.0140
Epoch 7, Batch 1550/3665: Loss=2.1404
Epoch 7, Batch 1600/3665: Loss=2.2109
Epoch 7, Batch 1650/3665: Loss=2.4403
Epoch 7, Batch 1700/3665: Loss=2.3255
Epoch 7, Batch 1750/3665: Loss=2.1089
Epoch 7, Batch 1800/3665: Loss=1.8136
Epoch 7, Batch 1850/3665: Loss=2.0634
Epoch 7, Batch 1900/3665: Loss=2.3606
Epoch 7, Batch 1950/3665: Loss=2.3221
Epoch 7, Batch 2000/3665: Loss=1.4561
Epoch 7, Batch 2050/3665: Loss=2.2322
Epoch 7, Batch 2100/3665: Loss=2.2084
Epoch 7, Batch 2150/3665: Loss=2.1758
Epoch 7, Batch 2200/3665: Loss=2.6938
Epoch 7, Batch 2250/3665: Loss=1.9215
Epoch 7, Batch 2300/3665: Loss=2.0036
Epoch 7, Batch 2350/3665: Loss=2.3323
Epoch 7, Batch 2400/3665: Loss=2.1300
Epoch 7, Batch 2450/3665: Loss=2.0525
Epoch 7, Batch 2500/3665: Loss=2.0534
Epoch 7, Batch 2550/3665: Loss=2.2570
Epoch 7, Batch 2600/3665: Loss=2.4526
Epoch 7, Batch 2650/3665: Loss=1.6778
Epoch 7, Batch 2700/3665: Loss=2.0003
Epoch 7, Batch 2750/3665: Loss=1.7592
Epoch 7, Batch 2800/3665: Loss=1.5519
Epoch 7, Batch 2850/3665: Loss=2.6465
Epoch 7, Batch 2900/3665: Loss=2.1193
Epoch 7, Batch 2950/3665: Loss=1.9474
Epoch 7, Batch 3000/3665: Loss=2.1168
Epoch 7, Batch 3050/3665: Loss=2.0564
Epoch 7, Batch 3100/3665: Loss=1.9181
Epoch 7, Batch 3150/3665: Loss=1.9005
Epoch 7, Batch 3200/3665: Loss=2.2153
Epoch 7, Batch 3250/3665: Loss=2.5122
Epoch 7, Batch 3300/3665: Loss=1.6777
Epoch 7, Batch 3350/3665: Loss=1.9567
Epoch 7, Batch 3400/3665: Loss=1.7397
Epoch 7, Batch 3450/3665: Loss=2.1825
Epoch 7, Batch 3500/3665: Loss=1.9460
Epoch 7, Batch 3550/3665: Loss=1.9533
Epoch 7, Batch 3600/3665: Loss=1.6797
Epoch 7, Batch 3650/3665: Loss=1.8881
Epoch 7 | Train Loss: 2.0054 | Val Accuracy: 41.68%
